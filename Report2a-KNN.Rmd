---
title: "KNN 2a"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r cars}
library(ggplot2)
library(caret)
library(leaps)
library(tidyverse)
library(forecast)
library(class)
library(FNN)

```

## Including Plots

You can also embed plots, for example:

```{r load the data and clean the data}
movie.t <- read.csv('/Users/blue/Desktop/Bigdata2/joindata2.csv')
# here, we use bo_and_num and runtimeMinutes try to predict whether the movie will become popular(average rating > 6.37)
movie.t$popularmovie <- ifelse(movie.t$averageRating > 6.37,1,0)
movie.t$bo_and_num <- movie.t$bo_year_rank * movie.t$numVotes
# use 1.5 iqr to exclude the outliars
str(movie.t)
q1 <- quantile(movie.t$numVotes,0.25)
q3 <- quantile(movie.t$numVotes,0.75)
iqr<- IQR(movie.t$numVotes)
movie.clean<-subset(movie.t, movie.t$numVotes  > (q1-1.5*iqr) & movie.t$numVotes  < (q3 +1.5*iqr))
# we can see after cleaning there are 2458 rows, original one has 2708 rows.
str(movie.clean)
```
```{r }
set.seed(2)
knn <- data.frame(movie.clean$bo_and_num,movie.clean$runtimeMinutes,movie.clean$popularmovie)
#random select 1000 rows of data
knn.dt <- sample_n(knn, 1000)
str(knn.dt)
hist(knn.dt$movie.clean.popularmovie)
#partition data
train.index <- sample(c(1:dim(knn.dt)[1]), dim(knn.dt)[1]*0.6) 
train.knn <- knn.dt[ train.index, ]
valid.knn <- knn.dt[ -train.index,]


```
```{r}
#### Table 7.2

# initialize normalized training, validation data, complete data frames to originals
train.norm.df <- train.knn
valid.norm.df <- valid.knn
movie.norm.df <- knn.dt
#use the scatter plot to show the distribution
plot(movie.clean.bo_and_num ~ movie.clean.runtimeMinutes, data=train.knn, pch=ifelse(train.knn$movie.clean.popularmovie=="1", 3, 6))
new.df <- data.frame(movie.clean.bo_and_num = 6000, movie.clean.runtimeMinutes = 150) # <<<<<<<<< new movie
text(6000, 150, "X")  
# use preProcess() from the caret package to normalize bo_and_num and runtimeMinutes.
norm.values <- preProcess(train.knn[, 1:2], method=c("center", "scale"))
train.norm.df[, 1:2] <- predict(norm.values, train.knn[, 1:2])
valid.norm.df[, 1:2] <- predict(norm.values, valid.knn[, 1:2])
movie.norm.df[, 1:2] <- predict(norm.values, knn.dt[, 1:2])  # whole thing
new.norm.df <- predict(norm.values, new.df)
```

```{r}
# Initialize a data frame with two columns: k, and accuracy.
accuracy.df <- data.frame(k = seq(1, 50, 1), accuracy = rep(0, 50))

# compute knn for different numbers of neighbords (k) on validation.
for(i in 1:50) {          # <<<< adjust the bounds to look at particular confusion matrix
  knn.pred <- knn(train = train.norm.df[, 1:2], cl = train.norm.df[, 3], 
                  test = valid.norm.df[, 1:2], k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred, factor(valid.norm.df[, 3]))$overall[1] 
  
}

# which k is the best? k=30(0.6975);k=28(0.6925)
accuracy.df

```
```{r}

for(i in 28:28) {  # <<<< adjust the bounds to look only at confusion matrix with specific k
  knn.pred <- knn(train = train.norm.df[, 1:2], cl = train.norm.df[, 3], 
                  test = valid.norm.df[, 1:2], k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred, factor(valid.norm.df[, 3]))$overall[1] 
  
  
}
# Confusion matrix
confusionMatrix(knn.pred, factor(valid.norm.df[, 3]))

```
k             30,      28,    
accuracy      0.6975 0.6925  
recall        0.7171 0.7073  
specificity   0.6769 0.6769 
f1 score      0.6964 0.6912  

we choose the k with highest accuracy 0.6975(30) and seconde high 0.6925(28), run the confusion matrix. From above we can see that k=30 has the highest f1 score, 0.6964.

since logistic model(without excluding outliars)'s accuracy and f1 score both are slightly greater than KNN model, we can conclude that the logistic model(without excluding outliars) preform best here.(accuracy = 0.7106; F1 score = 0.71; recall = 0.6996 ;  specificity=0.7196)

